DML2.for.PLM <- function(x, d, y, dreg, yreg, nfold=2) {
nobs <- nrow(x) #number of observations
foldid <- rep.int(1:nfold,times = ceiling(nobs/nfold))[sample.int(nobs)] #define folds indices
I <- split(1:nobs, foldid)  #split observation indices into folds
ytil <- dtil <- rep(NA, nobs)
cat("fold: ")
for(b in 1:length(I)){
dfit <- dreg(x[-I[[b]],], d[-I[[b]]]) #take a fold out
yfit <- yreg(x[-I[[b]],], y[-I[[b]]]) # take a foldt out
dhat <- predict(dfit, x[I[[b]],], type="response") #predict the left-out fold
yhat <- predict(yfit, x[I[[b]],], type="response") #predict the left-out fold
dtil[I[[b]]] <- (d[I[[b]]] - dhat) #record residual for the left-out fold
ytil[I[[b]]] <- (y[I[[b]]] - yhat) #record residial for the left-out fold
cat(b," ")
}
rfit <- lm(ytil ~ dtil)    #estimate the main parameter by regressing one residual on the other
coef.est <- coef(rfit)[2]  #extract coefficient
se <- sqrt(vcovHC(rfit)[2,2]) #record robust standard error
cat(sprintf("\ncoef (se) = %g (%g)\n", coef.est , se))  #printing output
return( list(coef.est =coef.est , se=se, dtil=dtil, ytil=ytil) ) #save output and residuals
}
data(GrowthData)                     # Barro-Lee growth data
y= as.matrix(GrowthData[,1])         # outcome: growth rate
d= as.matrix(GrowthData[,3])         # treatment: initial wealth
x= as.matrix(GrowthData[,-c(1,2,3)]) # controls: country characteristics
cat(sprintf("\n length of y is %g \n", length(y) ))
cat(sprintf("\n num features x is %g \n", dim(x)[2] ))
#summary(y)
#summary(d)
#summary(x)
cat(sprintf("\n Naive OLS that uses all features w/o cross-fitting \n"))
lres=summary(lm(y~d +x))$coef[2,1:2]
cat(sprintf("\ncoef (se) = %g (%g)\n", lres[1] , lres[2]))
#DML with OLS
cat(sprintf("\n DML with OLS w/o feature selection \n"))
set.seed(1)
dreg <- function(x,d){ glmnet(x, d, lambda = 0) } #ML method= OLS using glmnet; using lm gives bugs
yreg <- function(x,y){ glmnet(x, y, lambda = 0) } #ML method = OLS
DML2.OLS = DML2.for.PLM(x, d, y, dreg, yreg, nfold=10)
#DML with Lasso:
cat(sprintf("\n DML with Lasso \n"))
set.seed(1)
dreg <- function(x,d){ rlasso(x,d, post=FALSE) } #ML method= lasso from hdm
yreg <- function(x,y){ rlasso(x,y, post=FALSE) } #ML method = lasso from hdm
DML2.lasso = DML2.for.PLM(x, d, y, dreg, yreg, nfold=10)
cat(sprintf("\n DML with Random Forest \n"))
#DML with Random Forest:
dreg <- function(x,d){ randomForest(x, d) } #ML method=Forest
yreg <- function(x,y){ randomForest(x, y) } #ML method=Forest
set.seed(1)
DML2.RF = DML2.for.PLM(x, d, y, dreg, yreg, nfold=10)
cat(sprintf("\n DML with Lasso/Random Forest \n"))
#DML MIX:
dreg <- function(x,d){ rlasso(x,d, post=FALSE) } #ML method=Forest
yreg <- function(x,y){ randomForest(x, y) } #ML method=Forest
set.seed(1)
DML2.RF = DML2.for.PLM(x, d, y, dreg, yreg, nfold=10)
prRes.D<- c( mean((DML2.OLS$dtil)^2), mean((DML2.lasso$dtil)^2), mean((DML2.RF$dtil)^2));
prRes.Y<- c(mean((DML2.OLS$ytil)^2), mean((DML2.lasso$ytil)^2),mean((DML2.RF$ytil)^2));
prRes<- rbind(sqrt(prRes.D), sqrt(prRes.Y));
rownames(prRes)<- c("RMSE D", "RMSE Y");
colnames(prRes)<- c("OLS", "Lasso", "RF")
print(prRes,digit=2)
cat(sprintf("\n length of y is %g \n", length(y) ))
cat(sprintf("\n num features x is %g \n", dim(x)[2] ))
cat(sprintf("\n Naive OLS that uses all features w/o cross-fitting \n"))
lres <- summary(lm(y~d +x))$coef[2,1:2]
cat(sprintf("\ncoef (se) = %g (%g)\n", lres[1] , lres[2]))
pacman::p_load(AER, randomForest, hdm, glmnet)
DML2.for.PLM <- function(x, d, y, dreg, yreg, nfold=2) {
nobs <- nrow(x) #number of observations
foldid <- rep.int(1:nfold,times = ceiling(nobs/nfold))[sample.int(nobs)] #define folds indices
I <- split(1:nobs, foldid)  #split observation indices into folds
ytil <- dtil <- rep(NA, nobs)
browser()
cat("fold: ")
for(b in 1:length(I)){
dfit <- dreg(x[-I[[b]],], d[-I[[b]]]) #take a fold out
yfit <- yreg(x[-I[[b]],], y[-I[[b]]]) # take a foldt out
dhat <- predict(dfit, x[I[[b]],], type="response") #predict the left-out fold
yhat <- predict(yfit, x[I[[b]],], type="response") #predict the left-out fold
dtil[I[[b]]] <- (d[I[[b]]] - dhat) #record residual for the left-out fold
ytil[I[[b]]] <- (y[I[[b]]] - yhat) #record residial for the left-out fold
cat(b," ")
}
rfit <- lm(ytil ~ dtil)    #estimate the main parameter by regressing one residual on the other
coef.est <- coef(rfit)[2]  #extract coefficient
se <- sqrt(vcovHC(rfit)[2,2]) #record robust standard error
cat(sprintf("\ncoef (se) = %g (%g)\n", coef.est , se))  #printing output
return( list(coef.est =coef.est , se=se, dtil=dtil, ytil=ytil) ) #save output and residuals
}
data(GrowthData)                     # Barro-Lee growth data
y <- as.matrix(GrowthData[,1])         # outcome: growth rate
d <- as.matrix(GrowthData[,3])         # treatment: initial wealth
x <- as.matrix(GrowthData[,-c(1,2,3)]) # controls: country characteristics
cat(sprintf("\n length of y is %g \n", length(y) ))
cat(sprintf("\n num features x is %g \n", dim(x)[2] ))
cat(sprintf("\n Naive OLS that uses all features w/o cross-fitting \n"))
lres <- summary(lm(y~d +x))$coef[2,1:2]
cat(sprintf("\ncoef (se) = %g (%g)\n", lres[1] , lres[2]))
#DML with OLS
cat(sprintf("\n DML with OLS w/o feature selection \n"))
set.seed(1)
dreg <- function(x,d){ glmnet(x, d, lambda = 0) } #ML method= OLS using glmnet; using lm gives bugs
yreg <- function(x,y){ glmnet(x, y, lambda = 0) } #ML method = OLS
DML2.OLS <- DML2.for.PLM(x, d, y, dreg, yreg, nfold=10)
pacman::p_load(AER, randomForest, hdm, glmnet)
DML2.for.PLM <- function(x, d, y, dreg, yreg, nfold=2) {
nobs <- nrow(x) #number of observations
foldid <- rep.int(1:nfold,times = ceiling(nobs/nfold))[sample.int(nobs)] #define folds indices
I <- split(1:nobs, foldid)  #split observation indices into folds
ytil <- dtil <- rep(NA, nobs)
cat("fold: ")
for(b in 1:length(I)){
dfit <- dreg(x[-I[[b]],], d[-I[[b]]]) #take a fold out
yfit <- yreg(x[-I[[b]],], y[-I[[b]]]) # take a foldt out
dhat <- predict(dfit, x[I[[b]],], type="response") #predict the left-out fold
yhat <- predict(yfit, x[I[[b]],], type="response") #predict the left-out fold
dtil[I[[b]]] <- (d[I[[b]]] - dhat) #record residual for the left-out fold
ytil[I[[b]]] <- (y[I[[b]]] - yhat) #record residial for the left-out fold
cat(b," ")
}
rfit <- lm(ytil ~ dtil)    #estimate the main parameter by regressing one residual on the other
coef.est <- coef(rfit)[2]  #extract coefficient
se <- sqrt(vcovHC(rfit)[2,2]) #record robust standard error
cat(sprintf("\ncoef (se) = %g (%g)\n", coef.est , se))  #printing output
return( list(coef.est =coef.est , se=se, dtil=dtil, ytil=ytil) ) #save output and residuals
}
data(GrowthData)                     # Barro-Lee growth data
y <- as.matrix(GrowthData[,1])         # outcome: growth rate
d <- as.matrix(GrowthData[,3])         # treatment: initial wealth
x <- as.matrix(GrowthData[,-c(1,2,3)]) # controls: country characteristics
cat(sprintf("\n length of y is %g \n", length(y) ))
cat(sprintf("\n num features x is %g \n", dim(x)[2] ))
cat(sprintf("\n Naive OLS that uses all features w/o cross-fitting \n"))
lres <- summary(lm(y~d +x))$coef[2,1:2]
cat(sprintf("\ncoef (se) = %g (%g)\n", lres[1] , lres[2]))
#DML with OLS
cat(sprintf("\n DML with OLS w/o feature selection \n"))
set.seed(1)
dreg <- function(x,d){ glmnet(x, d, lambda = 0) } #ML method= OLS using glmnet; using lm gives bugs
yreg <- function(x,y){ glmnet(x, y, lambda = 0) } #ML method = OLS
DML2.OLS <- DML2.for.PLM(x, d, y, dreg, yreg, nfold=10)
cat(sprintf("\n DML with Lasso \n"))
set.seed(1)
dreg <- function(x,d){ rlasso(x,d, post=FALSE) } #ML method= lasso from hdm
yreg <- function(x,y){ rlasso(x,y, post=FALSE) } #ML method = lasso from hdm
DML2.lasso = DML2.for.PLM(x, d, y, dreg, yreg, nfold=10)
cat(sprintf("\n DML with Random Forest \n"))
dreg <- function(x,d){ randomForest(x, d) } #ML method=Forest
yreg <- function(x,y){ randomForest(x, y) } #ML method=Forest
set.seed(1)
DML2.RF = DML2.for.PLM(x, d, y, dreg, yreg, nfold=10)
cat(sprintf("\n DML with Lasso/Random Forest \n"))
dreg <- function(x,d){ rlasso(x,d, post=FALSE) } #ML method=Forest
yreg <- function(x,y){ randomForest(x, y) } #ML method=Forest
set.seed(1)
DML2.RF <- DML2.for.PLM(x, d, y, dreg, yreg, nfold=10)
n <- 1000
p <- 0.3
theta <- p*(1-p)
n <- 1000
p <- 0.3
theta <- p*(1-p)
X <- rbinom(n, 1, p)
U <- 0
for (i in seq_len(length(X))) {
for (j in seq_len(i-1)) {
U <- U + X[i]*(1-X[j])
}
}
U/choose(n, 2)
n <- 10000
p <- 0.3
theta <- p*(1-p)
X <- rbinom(n, 1, p)
U <- 0
for (i in seq_len(length(X))) {
for (j in seq_len(i-1)) {
U <- U + X[i]*(1-X[j])
}
}
U/choose(n, 2)
rm(list=ls())
pacma::p_load(foreign, quantreg, splines, lattice, Hmisc, fda, hdm,
randomForest, ranger, sandwich)
rm(list=ls())
pacman::p_load(foreign, quantreg, splines, lattice, Hmisc, fda, hdm,
randomForest, ranger, sandwich)
data(pension)
pension$net_tfa<-pension$net_tfa/10000
pension$inc = log(pension$inc)
#pension$inc[is.na(pension$inc)]<-0
pension<-pension[!is.na(pension$inc) & pension$inc!=-Inf & pension$inc !=Inf,]
Y=pension$net_tfa
## binary treatment --  indicator of 401(k) eligibility
D=pension$e401
X=pension$inc
Z = pension[,c("age","inc","fsize","educ","male","db","marr","twoearn",
"pira","hown","hval","hequity","hmort",
"nohs","hs","smcol")]
y_name   <- "net_tfa";
d_name    <- "e401";
form_z    <- "(poly(age, 6) + poly(inc, 8) + poly(educ, 4) + poly(fsize,2) + as.factor(marr) + as.factor(twoearn) + as.factor(db) + as.factor(pira) + as.factor(hown))^2";
first_stage_lasso<-function(data,d_name,y_name, form_z, seed=1) {
# Sample size
N<-dim(data)[1]
# Estimated regression function in control group
mu0.hat<-rep(1,N)
# Estimated regression function in treated group
mu1.hat<-rep(1,N)
# Propensity score
s.hat<-rep(1,N)
seed=1
## define sample splitting
set.seed(seed)
inds.train=sample(1:N,floor(N/2))
inds.eval=setdiff(1:N,inds.train)
print ("Estimate treatment probability, first half")
## conditional probability of 401 k eligibility (i.e., propensity score) based on random forest
fitted.lasso.pscore<-rlassologit(as.formula(paste0(d_name,"~",form_z )),data=data[inds.train,])
s.hat[inds.eval]<-predict(fitted.lasso.pscore,data[inds.eval,],type="response")
print ("Estimate treatment probability, second half")
fitted.lasso.pscore<-rlassologit(as.formula(paste0(d_name,"~",form_z )),data=data[inds.eval,])
s.hat[inds.train]<-predict( fitted.lasso.pscore,data[inds.train,],type="response")
data1<-data
data1[,d_name]<-1
data0<-data
data0[,d_name]<-0
print ("Estimate expectation function, first half")
fitted.lasso.mu<-rlasso(as.formula(paste0(y_name,"~",d_name,"+(",form_z,")" )),data=data[inds.train,])
mu1.hat[inds.eval]<-predict( fitted.lasso.mu,data1[inds.eval,])
mu0.hat[inds.eval]<-predict( fitted.lasso.mu,data0[inds.eval,])
print ("Estimate expectation function, second half")
fitted.lasso.mu<-rlasso(as.formula(paste0(y_name,"~",d_name,"+(",form_z,")" )),data=data[inds.eval,])
mu1.hat[inds.train]<-predict( fitted.lasso.mu,data1[inds.train,])
mu0.hat[inds.train]<-predict( fitted.lasso.mu,data0[inds.train,])
return (list(mu1.hat=mu1.hat,
mu0.hat=mu0.hat,
s.hat=s.hat))
}
first_stage_rf<-function(Y,D,Z,seed=1) {
# Sample size
N<-length(D)
# Estimated regression function in control group
mu0.hat<-rep(1,N)
# Estimated regression function in treated group
mu1.hat<-rep(1,N)
# Propensity score
s.hat<-rep(1,N)
## define sample splitting
set.seed(seed)
inds.train=sample(1:N,floor(N/2))
inds.eval=setdiff(1:N,inds.train)
print ("Estimate treatment probability, first half")
## conditional probability of 401 k eligibility (i.e., propensity score) based on random forest
D.f<-as.factor(as.character(D))
fitted.rf.pscore<-randomForest(Z,D.f,subset=inds.train)
s.hat[inds.eval]<-predict(fitted.rf.pscore,Z[inds.eval,],type="prob")[,2]
print ("Estimate treatment probability, second half")
fitted.rf<-randomForest(Z,D.f,subset=inds.eval)
s.hat[inds.train]<-predict(fitted.rf.pscore,Z[inds.train,],type="prob")[,2]
## conditional expected net financial assets (i.e.,  regression function) based on random forest
covariates<-cbind(Z,D)
covariates1<-cbind(Z,D=rep(1,N))
covariates0<-cbind(Z,D=rep(0,N))
print ("Estimate expectation function, first half")
fitted.rf.mu<-randomForest(cbind(Z,D),Y,subset=inds.train)
mu1.hat[inds.eval]<-predict( fitted.rf.mu,covariates1[inds.eval,])
mu0.hat[inds.eval]<-predict( fitted.rf.mu,covariates0[inds.eval,])
print ("Estimate expectation function, second half")
fitted.rf.mu<-randomForest(cbind(Z,D),Y,subset=inds.eval)
mu1.hat[inds.train]<-predict( fitted.rf.mu,covariates1[inds.train,])
mu0.hat[inds.train]<-predict( fitted.rf.mu,covariates0[inds.train,])
return (list(mu1.hat=mu1.hat,
mu0.hat=mu0.hat,
s.hat=s.hat))
}
fs.hat.rf = first_stage_rf(Y,D,Z)
qtmax <- function(C, S=10000, alpha)
{;
p <- nrow(C);
tmaxs <- apply(abs(matrix(rnorm(p*S), nrow = p, ncol = S)), 2, max);
return(quantile(tmaxs, 1-alpha));
};
# This function computes the square root of a symmetric matrix using the spectral decomposition;
group_average_treatment_effect<-function(X,Y,max_grid=5,alpha=0.05, B=10000) {
grid<-quantile(X,probs=c((0:max_grid)/max_grid))
X.raw<-matrix(NA, nrow=length(Y),ncol=length(grid)-1)
for (k in 2:((length(grid)))) {
X.raw[,k-1]<-sapply(X, function (x) ifelse (x>=grid[k-1] & x<grid[k],1,0) )
}
k=length(grid)
X.raw[,k-1]<-sapply(X, function (x) ifelse (x>=grid[k-1] & x<=grid[k],1,0) )
ols.fit<- lm(Y~X.raw-1)
coefs   <- coef(ols.fit)
vars <- names(coefs)
HCV.coefs <- vcovHC(ols.fit, type = 'HC')
coefs.se <- sqrt(diag(HCV.coefs)) # White std errors
## this is an identity matrix
## qtmax is simplified
C.coefs  <- (diag(1/sqrt(diag(HCV.coefs)))) %*% HCV.coefs %*% (diag(1/sqrt(diag(HCV.coefs))));
tes  <- coefs
tes.se <- coefs.se
tes.cor <- C.coefs
crit.val <- qtmax(tes.cor,B,alpha);
tes.ucb  <- tes + crit.val * tes.se;
tes.lcb  <- tes - crit.val * tes.se;
tes.uci  <- tes + qnorm(1-alpha/2) * tes.se;
tes.lci  <- tes + qnorm(alpha/2) * tes.se;
return(list(beta.hat=coefs, ghat.lower.point=tes.lci, ghat.upper.point=tes.uci,
ghat.lower=tes.lcb, ghat.upper= tes.ucb, crit.val=crit.val ))
}
res<-group_average_treatment_effect(X=X,Y=RobustSignal)
X=pension$inc
fs.hat<-fs.hat.rf
min_cutoff=0.01
# regression function
mu1.hat<-fs.hat[["mu1.hat"]]
mu0.hat<-fs.hat[["mu0.hat"]]
# propensity score
s.hat<-fs.hat[["s.hat"]]
s.hat<-sapply(s.hat,max,min_cutoff)
### Construct Orthogonal Signal
RobustSignal<-(Y - mu1.hat)*D/s.hat - (Y - mu0.hat)*(1-D)/(1-s.hat) + mu1.hat - mu0.hat
qtmax <- function(C, S=10000, alpha)
{;
p <- nrow(C);
tmaxs <- apply(abs(matrix(rnorm(p*S), nrow = p, ncol = S)), 2, max);
return(quantile(tmaxs, 1-alpha));
};
# This function computes the square root of a symmetric matrix using the spectral decomposition;
group_average_treatment_effect<-function(X,Y,max_grid=5,alpha=0.05, B=10000) {
grid<-quantile(X,probs=c((0:max_grid)/max_grid))
X.raw<-matrix(NA, nrow=length(Y),ncol=length(grid)-1)
for (k in 2:((length(grid)))) {
X.raw[,k-1]<-sapply(X, function (x) ifelse (x>=grid[k-1] & x<grid[k],1,0) )
}
k=length(grid)
X.raw[,k-1]<-sapply(X, function (x) ifelse (x>=grid[k-1] & x<=grid[k],1,0) )
ols.fit<- lm(Y~X.raw-1)
coefs   <- coef(ols.fit)
vars <- names(coefs)
HCV.coefs <- vcovHC(ols.fit, type = 'HC')
coefs.se <- sqrt(diag(HCV.coefs)) # White std errors
## this is an identity matrix
## qtmax is simplified
C.coefs  <- (diag(1/sqrt(diag(HCV.coefs)))) %*% HCV.coefs %*% (diag(1/sqrt(diag(HCV.coefs))));
tes  <- coefs
tes.se <- coefs.se
tes.cor <- C.coefs
crit.val <- qtmax(tes.cor,B,alpha);
tes.ucb  <- tes + crit.val * tes.se;
tes.lcb  <- tes - crit.val * tes.se;
tes.uci  <- tes + qnorm(1-alpha/2) * tes.se;
tes.lci  <- tes + qnorm(alpha/2) * tes.se;
return(list(beta.hat=coefs, ghat.lower.point=tes.lci, ghat.upper.point=tes.uci,
ghat.lower=tes.lcb, ghat.upper= tes.ucb, crit.val=crit.val ))
}
res<-group_average_treatment_effect(X=X,Y=RobustSignal)
## this code is taken from L1 14.382 taught at MIT
## author: Mert Demirer
options(repr.plot.width=10, repr.plot.height=8)
tes<-res$beta.hat
tes.lci<-res$ghat.lower.point
tes.uci<-res$ghat.upper.point
tes.lcb<-res$ghat.lower
tes.ucb<-res$ghat.upper
tes.lev<-c('0%-20%', '20%-40%','40%-60%','60%-80%','80%-100%')
plot( c(1,5), las = 2, xlim =c(0.6, 5.4), ylim = c(.05, 2.09),  type="n",xlab="Income group",
ylab="Average Effect on NET TFA (per 10 K)", main="Group Average Treatment Effects on NET TFA", xaxt="n");
axis(1, at=1:5, labels=tes.lev);
for (i in 1:5)
{;
rect(i-0.2, tes.lci[i], i+0.2,  tes.uci[i], col = NA,  border = "red", lwd = 3);
rect(i-0.2, tes.lcb[i], i+0.2, tes.ucb[i], col = NA,  border = 4, lwd = 3 );
segments(i-0.2, tes[i], i+0.2, tes[i], lwd = 5 );
};
abline(h=0);
legend(2.5, 2.0, c('Regression Estimate', '95% Simultaneous Confidence Interval', '95% Pointwise Confidence Interval'), col = c(1,4,2), lwd = c(4,3,3), horiz = F, bty = 'n', cex=0.8);
group_average_treatment_effect<-function(X,Y,max_grid=5,alpha=0.05, B=10000) {
grid<-quantile(X,probs=c((0:max_grid)/max_grid))
X.raw<-matrix(NA, nrow=length(Y),ncol=length(grid)-1)
for (k in 2:((length(grid)))) {
X.raw[,k-1]<-sapply(X, function (x) ifelse (x>=grid[k-1] & x<grid[k],1,0) )
}
k=length(grid)
X.raw[,k-1]<-sapply(X, function (x) ifelse (x>=grid[k-1] & x<=grid[k],1,0) )
browser()
ols.fit<- lm(Y~X.raw-1)
coefs   <- coef(ols.fit)
vars <- names(coefs)
HCV.coefs <- vcovHC(ols.fit, type = 'HC')
coefs.se <- sqrt(diag(HCV.coefs)) # White std errors
## this is an identity matrix
## qtmax is simplified
C.coefs  <- (diag(1/sqrt(diag(HCV.coefs)))) %*% HCV.coefs %*% (diag(1/sqrt(diag(HCV.coefs))));
tes  <- coefs
tes.se <- coefs.se
tes.cor <- C.coefs
crit.val <- qtmax(tes.cor,B,alpha);
tes.ucb  <- tes + crit.val * tes.se;
tes.lcb  <- tes - crit.val * tes.se;
tes.uci  <- tes + qnorm(1-alpha/2) * tes.se;
tes.lci  <- tes + qnorm(alpha/2) * tes.se;
return(list(beta.hat=coefs, ghat.lower.point=tes.lci, ghat.upper.point=tes.uci,
ghat.lower=tes.lcb, ghat.upper= tes.ucb, crit.val=crit.val ))
}
res<-group_average_treatment_effect(X=X,Y=RobustSignal)
least_squares_splines<-function(X,Y,max_knot=9,norder,nderiv,...) {
## Create technical regressors
cv.bsp<-rep(0,max_knot-1)
for (knot in 2:max_knot) {
breaks<- quantile(X, c(0:knot)/knot)
formula.bsp 	<- Y ~ bsplineS(X, breaks =breaks, norder = norder, nderiv = nderiv)[ ,-1]
fit	<- lm(formula.bsp);
cv.bsp[knot-1]		<- sum( (fit$res / (1 - hatvalues(fit)) )^2);
}
## Number of knots chosen by cross-validation
cv_knot<-which.min(cv.bsp)+1
breaks<- quantile(X, c(0:cv_knot)/cv_knot)
formula.bsp 	<- Y ~ bsplineS(X, breaks =breaks, norder = norder, nderiv = 0)[ ,-1]
fit	<- lm(formula.bsp);
return(list(cv_knot=cv_knot,fit=fit))
}
least_squares_series<-function(X, Y,max_degree,...) {
cv.pol<-rep(0,max_degree)
for (degree in 1:max_degree) {
formula.pol 	<- Y ~ poly(X, degree)
fit	<- lm(formula.pol );
cv.pol[degree]		<- sum( (fit$res / (1 - hatvalues(fit)) )^2);
}
## Number of knots chosen by cross-validation
cv_degree<-which.min(cv.pol)
## Estimate coefficients
formula.pol 	<- Y ~ poly(X, cv_degree)
fit	<- lm(formula.pol);
return(list(fit=fit,cv_degree=cv_degree))
}
msqrt <- function(C)
{;
C.eig <- eigen(C);
return(C.eig$vectors %*% diag(sqrt(C.eig$values)) %*% solve(C.eig$vectors));
};
tboot<-function(regressors_grid, Omega.hat ,alpha, B=10000) {
numerator_grid<-regressors_grid%*%msqrt( Omega.hat)
denominator_grid<-sqrt(diag(regressors_grid%*% Omega.hat%*%t(regressors_grid)))
norm_numerator_grid<-numerator_grid
for (k in 1:dim(numerator_grid)[1]) {
norm_numerator_grid[k,]<-numerator_grid[k,]/denominator_grid[k]
}
tmaxs <- apply(abs( norm_numerator_grid%*% matrix(rnorm(dim(numerator_grid)[2]*B), nrow = dim(numerator_grid)[2], ncol = B)), 2, max)
return(quantile(tmaxs, 1-alpha))
}
second_stage<-function(fs.hat,Y,D,X,max_degree=3,norder=4,nderiv=0,ss_method="poly",min_cutoff=0.01,alpha=0.05,eps=0.1,...) {
X_grid = seq(min(X),max(X),eps)
mu1.hat<-fs.hat[["mu1.hat"]]
mu0.hat<-fs.hat[["mu0.hat"]]
# propensity score
s.hat<-fs.hat[["s.hat"]]
s.hat<-sapply(s.hat,max,min_cutoff)
### Construct Orthogonal Signal
RobustSignal<-(Y - mu1.hat)*D/s.hat - (Y - mu0.hat)*(1-D)/(1-s.hat) + mu1.hat - mu0.hat
# Estimate the target function using least squares series
if (ss_method == "ortho_poly") {
res<-least_squares_series(X=X,Y=RobustSignal,eps=0.1,max_degree=max_degree)
fit<-res$fit
cv_degree<-res$cv_degree
regressors_grid<-cbind( rep(1,length(X_grid)), poly(X_grid,cv_degree))
}
if (ss_method == "splines") {
res<-least_squares_splines(X=X,Y=RobustSignal,eps=0.1,norder=norder,nderiv=nderiv)
fit<-res$fit
cv_knot<-res$cv_knot
breaks<- quantile(X, c(0:cv_knot)/cv_knot)
regressors_grid<-cbind( rep(1,length(X_grid)), bsplineS(X_grid, breaks =breaks, norder = norder, nderiv = nderiv)[ ,-1])
degree=cv_knot
}
g.hat<-regressors_grid%*%coef(fit)
HCV.coefs <- vcovHC(fit, type = 'HC')
#Omega.hat<-white_vcov(regressors,Y,b.hat=coef(fit))
standard_error<-sqrt(diag(regressors_grid%*% HCV.coefs%*%t(regressors_grid)))
### Lower Pointwise CI
ghat.lower.point<-g.hat+qnorm(alpha/2)*standard_error
### Upper Pointwise CI
ghat.upper.point<-g.hat+qnorm(1-alpha/2)*standard_error
max_tstat<-tboot(regressors_grid=regressors_grid,  Omega.hat=HCV.coefs,alpha=alpha)
## Lower Uniform CI
ghat.lower<-g.hat-max_tstat*standard_error
## Upper Uniform CI
ghat.upper<-g.hat+max_tstat*standard_error
return(list(ghat.lower=ghat.lower,g.hat=g.hat, ghat.upper=ghat.upper,fit=fit,ghat.lower.point=ghat.lower.point,
ghat.upper.point=ghat.upper.point,X_grid=X_grid))
}
?CVXR::solve
#########################################################################################
## Remember to check often that all the packages are updated!
#old.packages()
#update.packages(ask = FALSE)
remove.packages("scpi")
pacman::p_load(devtools, testthat)
#########################################################################################
## Change Version Below!!!
setwd("/Users/fpalomba/Dropbox (Princeton)/projects/scpi/packages/R/scpi")
# Prepare build ignore
usethis::use_build_ignore(c("tests", ".gitignore"))
#usethis::use_github_action_check_standard()
# Prepare documentation
devtools::document()
# Install and check
devtools::build()
devtools::install(upgrade = "never")
